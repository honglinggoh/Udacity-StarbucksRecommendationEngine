{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, f1_score\n",
    "\n",
    "#import clustering libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "def offer_mapper(df, source='offer_id', target='offer_label'):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    - df - input data frame with offer_id to map\n",
    "    Output:\n",
    "    - df - updated offer_id to simpler identifier\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # map the offer ids to identifier\n",
    "    offer_dict = {'ae264e3637204a6fb9bb56bc8210ddfd': 'B1',\n",
    "                '4d5c57ea9a6940dd891ad53e9dbe8da0': 'B2',\n",
    "                '9b98b8c7a33c4b65b9aebfe6a799e6d9': 'B3',\n",
    "                'f19421c1d4aa40978ebb69ca19b0e20d': 'B4',\n",
    "                '0b1e1539f2cc45b7b9fa7c272da2e1d7': 'D1',\n",
    "                '2298d6c36e964ae4a3e7e9706d1fb8c2': 'D2',\n",
    "                'fafdcd668e3743c1bb461111dcafc2a4': 'D3',\n",
    "                '2906b810c7d4411798c6938adc9daaa5': 'D4',\n",
    "                '3f207df678b143eea3cee63160fa8bed': 'I1',\n",
    "                '5a8bc65990b245e5a138643cd4eb9837': 'I2'}\n",
    "    df[target] = df[source].apply(lambda x: offer_dict[x] if x else None)\n",
    "    return df\n",
    "\n",
    "def offer_mapper_reverse(df, source='offer_label', target='offer_id'):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    - df - input data frame with offer_id to reverse mapping\n",
    "    Output:\n",
    "    - df - updated offer_id to original identifier\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # map the identifier to offer_id\n",
    "    offer_dict = {'B1': 'ae264e3637204a6fb9bb56bc8210ddfd',\n",
    "                'B2': '4d5c57ea9a6940dd891ad53e9dbe8da0',\n",
    "                'B3': '9b98b8c7a33c4b65b9aebfe6a799e6d9',\n",
    "                'B4': 'f19421c1d4aa40978ebb69ca19b0e20d',\n",
    "                'D1': '0b1e1539f2cc45b7b9fa7c272da2e1d7',\n",
    "                'D2': '2298d6c36e964ae4a3e7e9706d1fb8c2',\n",
    "                'D3': 'fafdcd668e3743c1bb461111dcafc2a4',\n",
    "                'D4': '2906b810c7d4411798c6938adc9daaa5',\n",
    "                'I1': '3f207df678b143eea3cee63160fa8bed',\n",
    "                'I2': '5a8bc65990b245e5a138643cd4eb9837'}\n",
    "    df[target] = df[source].apply(lambda x: offer_dict[x] if x else None)\n",
    "    return df\n",
    "\n",
    "def clean_portfolio(portfolio):\n",
    "    '''\n",
    "    INPUT \n",
    "        portfolio - raw portfolio dataframe\n",
    "    OUTPUT\n",
    "        df_portfolio_clean - Returns the processed portfolio dataframe after data cleansing\n",
    "    '''\n",
    "    # check the # of rows and columns\n",
    "    #portfolio.shape\n",
    "\n",
    "    # Create dummy columns for the channels column\n",
    "    df_channel = pd.get_dummies(portfolio['channels'].apply(pd.Series).stack(),\n",
    "                                prefix=\"channel\").sum(level=0)\n",
    "\n",
    "    # concat the dummy columns with the portfolio data frame\n",
    "    df_portfolio_clean = pd.concat([portfolio, df_channel], axis=1)\n",
    "\n",
    "    # drop the channels column which is no longer needed with the dummy columns creation\n",
    "    df_portfolio_clean.drop(columns='channels', inplace=True)\n",
    "\n",
    "    # rename the column to make it more identifiable\n",
    "    df_portfolio_clean.rename(columns={'id':'offer_id'}, inplace=True)\n",
    "    \n",
    "    # map the offer_id to simpler identifier to ease the processing later\n",
    "    df_portfolio_clean = offer_mapper(df_portfolio_clean, source='offer_id', target='offer_label')\n",
    "\n",
    "    return df_portfolio_clean\n",
    "\n",
    "\n",
    "def clean_profile(profile):\n",
    "    '''\n",
    "    INPUT \n",
    "        profile - raw profile dataframe\n",
    "    OUTPUT\n",
    "        df_profile_clean - Returns the processed profile dataframe after data cleansing\n",
    "    '''\n",
    "    # check the # of rows and columns\n",
    "    #profile.shape\n",
    "    \n",
    "    # Analyze the NaN value in the profile dataset.\n",
    "    # only gender & income columns has NaN value\n",
    "    profile.isna().sum()\n",
    "    \n",
    "    # make a copy of the profile data frame\n",
    "    df_profile_clean = profile.copy()\n",
    "    \n",
    "    # Fillna for NaN value in gender and income columns\n",
    "    df_profile_clean.gender.fillna(value='N', inplace=True)\n",
    "    df_profile_clean.income.fillna(value=0, inplace=True)\n",
    "\n",
    "    # rename the column to make it more identifiable\n",
    "    df_profile_clean.rename(columns={'id':'customer_id'}, inplace=True)   \n",
    "    \n",
    "    # change the became_member_on to valid date format\n",
    "    df_profile_clean['became_member_on'] =pd.to_datetime(df_profile_clean['became_member_on'], format='%Y%m%d')\n",
    "\n",
    "    # base on the dataset, seems like whenever age=118, gender & income data will not be available at the same time\n",
    "    # hence, we can create a new column to separate customer with & without the complete profile information\n",
    "    df_profile_clean['complete_profile'] = (df_profile_clean['gender'] != 'N').astype(int)\n",
    "    \n",
    "    # map the gender to int category\n",
    "    dict_gender = {\n",
    "        'N': 0,\n",
    "        'F': 1,\n",
    "        'M': 2,\n",
    "        'O': 3 \n",
    "    }\n",
    "    df_profile_clean['gender_cat']=df_profile_clean['gender'].apply(lambda x: dict_gender[x])\n",
    "    \n",
    "    # Age Group Categorization - from https://v12data.com/blog/generational-consumer-shopping-trends/\n",
    "    # 1 - GenZ (younger than 25)\n",
    "    # 2 - Millennials (25-35)\n",
    "    # 3 - GenX (36-54)\n",
    "    # 4 - Boomers (55-75)\n",
    "    # 5 - Silents(76+)\n",
    "    # 99 - Invalid age\n",
    "    age_ul = [15, 25, 35, 54, 75, 105, 120]\n",
    "    age_cat = ['1','2','3','4','5','0']\n",
    "    df_profile_clean['age_cat']=pd.cut(df_profile_clean['age'].values, age_ul , labels = age_cat)\n",
    "    df_profile_clean['age_cat']=df_profile_clean['age_cat'].astype(int)\n",
    "\n",
    "    # Check on income summary stats\n",
    "    df_profile_clean['income'].describe()\n",
    "\n",
    "    # Salary Categorization - range set based on the income summary stats above\n",
    "    # 1 - 29,001 - 45,000\n",
    "    # 2 - 45,001 - 60,000\n",
    "    # 3 - 60,001 - 75,000\n",
    "    # 4 - 75,001 - 90,000\n",
    "    # 5 - 90,001 - 105,000\n",
    "    # 6 - 105,001 and above\n",
    "    income_ul = [-1, 29000, 45000, 60000, 75000, 90000, 105000, 125000]\n",
    "    income_cat = ['0','1','2','3','4','5','6']\n",
    "    df_profile_clean['income_cat']=pd.cut(df_profile_clean['income'].values, income_ul , labels = income_cat)\n",
    "    df_profile_clean['income_cat']=df_profile_clean['income_cat'].astype(int)\n",
    "    \n",
    "    return df_profile_clean\n",
    "\n",
    "\n",
    "def clean_transcript(transcript):\n",
    "    '''\n",
    "    INPUT \n",
    "        transcript - raw transcript dataframe\n",
    "    OUTPUT\n",
    "        df_transcript_clean - Returns the processed transcript dataframe after data cleansing\n",
    "    '''\n",
    "\n",
    "    # check the # of rows and columns\n",
    "    #transcript.shape   \n",
    "    \n",
    "    # Analyze the NaN value in the transcript dataset.\n",
    "    # No Nan Value found.\n",
    "    transcript.isna().sum()\n",
    "    \n",
    "    # make a copy of the transcript data frame\n",
    "    df_transcript_clean = transcript.copy()\n",
    "\n",
    "    # rename the column to make it more identifiable\n",
    "    df_transcript_clean.rename(columns={'person':'customer_id'}, inplace=True)  \n",
    "\n",
    "    # prepare the event name for get_dummies function\n",
    "    df_transcript_clean.event.unique()\n",
    "    df_transcript_clean['event'] = df_transcript_clean.event.str.replace(' ','_')\n",
    "    \n",
    "    # Create dummy columns for the event column\n",
    "    df_event = pd.get_dummies(df_transcript_clean['event'], prefix=\"event\")\n",
    "\n",
    "    # concat the dummy columns with the transcript data frame\n",
    "    df_transcript_clean = pd.concat([df_transcript_clean, df_event], axis=1)\n",
    "\n",
    "    # drop the event column which is no longer needed with the dummy columns creation\n",
    "    df_transcript_clean.drop(columns='event', inplace=True)\n",
    "\n",
    "    # create new column = offer_id to store the offer id value\n",
    "    df_transcript_clean['offer_id'] = [[*v.values()][0]\n",
    "                                    if [*v.keys()][0] in ['offer id','offer_id'] else None\n",
    "                                    for v in df_transcript_clean.value]\n",
    "\n",
    "    # create new column = amount to store the amount value\n",
    "    df_transcript_clean['amount'] = [np.round([*v.values()][0], decimals=2)\n",
    "                                    if [*v.keys()][0] == 'amount' else None\n",
    "                                    for v in df_transcript_clean.value]\n",
    "\n",
    "    # drop the value column which is no longer needed with the offer_id and amount columns creation\n",
    "    df_transcript_clean.drop(columns='value', inplace=True)\n",
    "\n",
    "    return df_transcript_clean\n",
    "\n",
    "def plt_data(df=None, figsize=(14,4), subplot=111, kind='bar', ylabel='', xlabel='', title='', plt_type=None, bins=10\\\n",
    "             , color='tab:blue', tableTF=True, gridTF=True, new_fig=True):\n",
    "    '''\n",
    "    INPUT \n",
    "        df - data frame for the graph plot\n",
    "        figsize - graph figure size\n",
    "        subplot - subplot number/position\n",
    "        kind - the type of graph to plot (ex: bar, line)\n",
    "        ylabel - y axis label to display\n",
    "        xlabel - x axis label to display\n",
    "        plt_type - whether it is hist or normal plot\n",
    "        bins - for hist, # of bins to display\n",
    "        color - graph plot color\n",
    "        tableTF - to display data table - True or False\n",
    "        gridTF - to display graph grid - True or False\n",
    "    OUTPUT\n",
    "        display plotted graph\n",
    "    '''\n",
    "    \n",
    "    if df is not None:\n",
    "        if new_fig:\n",
    "            plt.figure(figsize=figsize)\n",
    "            \n",
    "        if subplot != 111:\n",
    "            plt.subplot(subplot)\n",
    "            \n",
    "        if plt_type is None:\n",
    "            df.plot(kind=kind, rot=0, figsize=figsize, color=color, table=tableTF)\n",
    "        else:\n",
    "            if plt_type == 'hist':\n",
    "                plt.hist(df, bins)\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.title(title)\n",
    "        plt.grid(gridTF)\n",
    "\n",
    "def get_top_offer_ids(n=3, df=None, offer_key='viewed'):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top offers id to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    offer_key - either it is received, viewed or completed\n",
    "   \n",
    "    OUTPUT:\n",
    "    top_offer_ids - (list) A list of the top 'n' offer ids \n",
    "    \n",
    "    '''\n",
    "    # retrieve a list of n top offer ids\n",
    "    if offer_key in ('received','viewed','completed'):\n",
    "        offer_trans = 'cnt_' + offer_key\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    top_offer_ids = list(df.groupby(['offer_label']).sum()[offer_trans].sort_values(ascending=False)[:n,].keys())\n",
    " \n",
    "    return top_offer_ids # Return the top offer ids\n",
    "\n",
    "\n",
    "def get_top_offers(customer_id=None, n=1, df=None, dfpf=None, dfmcot=None, dfmco=None, offer_key='viewed'):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top offers to return\n",
    "    df - portfolio dataframe\n",
    "    dfpf - profile data frame\n",
    "    dfmcot - all data sets merged data frame\n",
    "    dfmco - offer transaction data frame\n",
    "    offer_key - either it is received, viewed or completed\n",
    "    \n",
    "    OUTPUT:\n",
    "    top_offers - A dataframe of the top 'n' offers details either based on Rank-Based or Collaborative-Filtering Recommendations\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if (customer_id != None):\n",
    "        # if customer id is provided, we will recommend based on collaborative filtering instead\n",
    "        top_offer_ids = get_top_offer_ids_customer(customer_id=customer_id, n=n, dfpf=dfpf, dfmcot=dfmcot, dfmco=dfmco, offer_key=offer_key)\n",
    "        if (top_offer_ids == None):\n",
    "            # if no recommendation return due to customer_id not existed before, will use rank-based recommendation\n",
    "            top_offer_ids = get_top_offer_ids(n=n, df=dfmco, offer_key=offer_key)\n",
    "    else:\n",
    "        # if it is new customer, will use rank-based recommendation\n",
    "        top_offer_ids = get_top_offer_ids(n=n, df=dfmco, offer_key=offer_key)\n",
    "            \n",
    "    # retrieve the n top offer as a list object\n",
    "    top_offers = df[df['offer_label'].isin(top_offer_ids)]\n",
    "    \n",
    "    return top_offers # Return the top offers from portfolio df\n",
    "\n",
    "def create_cluster(dfmcot=None):\n",
    "    '''\n",
    "    INPUT\n",
    "    dfmcot - the summarized offers and purchase activities by customer \n",
    "\n",
    "    OUTPUT\n",
    "    cluster_list - the list of clusters for customers \n",
    "    '''\n",
    "    \n",
    "    df_cluster = dfmcot.copy()\n",
    "    # remove the non numerics columns from the data frame\n",
    "    df_cluster.drop(['customer_id','gender'], axis=1, inplace=True)\n",
    "    \n",
    "    #scale the data using standard scaler\n",
    "    scale = StandardScaler()\n",
    "    scale.fit(df_cluster[['sum_amount','avg_purchase']])\n",
    "    data = scale.transform(df_cluster[['sum_amount','avg_purchase']])\n",
    "    df_scaled = df_cluster.copy()\n",
    "    df_scaled['sum_amount'] = data[:, 0]\n",
    "    df_scaled['avg_purchase'] = data[:, 1]\n",
    "    \n",
    "    # use LabelEncoder to encode age_cat and income_cat from categorical value to int\n",
    "    le_number = LabelEncoder()\n",
    "    df_scaled['age_cat'] = le_number.fit_transform(df_scaled['age_cat'])\n",
    "    df_scaled['income_cat'] = le_number.fit_transform(df_scaled['income_cat'])\n",
    "\n",
    "    #fit KMeans for every k, append the SSE score to a list (scores)\n",
    "    scores = []\n",
    "    k_list = list(range(1,25))\n",
    "    for k in k_list:\n",
    "        kmeans = KMeans(k)\n",
    "        model = kmeans.fit(df_scaled)\n",
    "        scores.append(np.abs(model.score(df_scaled)))\n",
    "\n",
    "    #plot the scores to find the elbow\n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.plot(k_list, scores, linestyle='--', marker='o', color='b');\n",
    "    plt.xlabel('K');\n",
    "    plt.ylabel('Score');\n",
    "    plt.xticks(k_list)\n",
    "    plt.title('SSE vs. K');\n",
    "\n",
    "    #predicting clusters for each users based on the optimum k which is in this case I choose 11\n",
    "    k = KMeans(11)\n",
    "    cluster_list = k.fit_predict(df_scaled)\n",
    "\n",
    "    #changing values in cluster_list so it starts from 1 \n",
    "    cluster_list = cluster_list + 1\n",
    "    \n",
    "    return cluster_list    \n",
    "\n",
    "def get_top_offer_ids_customer (customer_id=None, n=3, dfpf=None, dfmcot=None, dfmco=None, offer_key='viewed'):\n",
    "        '''\n",
    "        INPUT\n",
    "        customer_id - customer id to recommend offer for\n",
    "        n - top n rank\n",
    "        dfpf - the clean profile dataframe\n",
    "        dfmcot - the summarized offers and purchase activities by customer \n",
    "        dfmco - the summarized offers activities by customer \n",
    "        offer_key = could be either received, viewed, completed\n",
    "    \n",
    "        OUTPUT\n",
    "        ranked_offers \n",
    "            - if customer profile exists:\n",
    "                - look for other customers within same cluster\n",
    "                - then look for all offer ids for the customers sorted by:\n",
    "                    highest average purchase, # of purchase transactions, count of offer viewed & completed \n",
    "                - return the offers list \n",
    "        '''\n",
    "\n",
    "        # check if customer_id provided\n",
    "        if (customer_id == None):\n",
    "            print('Customer id none')\n",
    "            return None\n",
    "        \n",
    "        # retrieve a list of n top offer ids\n",
    "        if offer_key in ('received','viewed','completed'):\n",
    "            offer_trans = 'cnt_' + offer_key\n",
    "        else:\n",
    "            print('Offer key none')\n",
    "            return None        \n",
    " \n",
    "\n",
    "        # pull the list of customers within the same cluster if exists\n",
    "        cluster_id = dfmcot[['cluster']][dfmcot['customer_id'] == customer_id].values\n",
    "        \n",
    "        if len(cluster_id) > 0:\n",
    "            ls_sim_customers = dfmcot.loc[dfmcot.cluster == int(cluster_id),'customer_id'].values.tolist()\n",
    "\n",
    "            if len(ls_sim_customers) > 0:\n",
    "                # remove the customer_id from the similar customer list\n",
    "                ls_sim_customers = list(filter((customer_id).__ne__, ls_sim_customers))\n",
    "\n",
    "                df_sim_customers = dfmcot.loc[dfmcot['customer_id'].isin(ls_sim_customers)]\n",
    "\n",
    "                 # sort by top avg purchase, cnt transacts(purchase), cnt viewed, cnt completed\n",
    "                df_sim_customers= df_sim_customers.sort_values(by=['avg_purchase', 'cnt_transaction', 'cnt_viewed', 'cnt_completed'],\\\n",
    "                                             ascending=False)       \n",
    "\n",
    "                k = int(df_sim_customers.shape[0]*(5/100))\n",
    "                # pull the list of offers for this top 5% customers\n",
    "                df_sim_cust_offer = dfmco.loc[dfmco['customer_id'].isin(list(df_sim_customers.customer_id[:k])), ['offer_label', offer_trans]]\\\n",
    "                                                   .groupby(['offer_label']).sum().sort_values(by=offer_trans, ascending=False)\n",
    "\n",
    "\n",
    "                # return the top n offer ids \n",
    "                return list(df_sim_cust_offer.index[:n])\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def fit(dfo=None, offer_key='viewed', latent_features=12, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "\n",
    "    INPUT:\n",
    "    dfo - the customer offer data frame\n",
    "    offer_key = could be either received, viewed, completed\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate\n",
    "    iters - (int) the number of iterations\n",
    "\n",
    "    OUTPUT:\n",
    "    user_mat - user matrix\n",
    "    offer_mat - offer matrix\n",
    "    user_ids_series - series of user_ids\n",
    "    offer_labels_series - series of offer_labels\n",
    "    '''\n",
    "    \n",
    "    if offer_key in ('received','viewed','completed'):\n",
    "        offer_trans = 'cnt_' + offer_key\n",
    "    else:\n",
    "        print('Invalid offer key')\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Create user-item matrix\n",
    "    offer_type = 'cnt_' + offer_key\n",
    "    usr_itm = dfo[['customer_id', 'offer_label', offer_type]]\n",
    "    user_item_df = usr_itm.groupby(['customer_id','offer_label'])[offer_type].max().unstack()\n",
    "    user_item_mat= np.array(user_item_df)\n",
    "\n",
    "    # parameters\n",
    "    latent_features = latent_features\n",
    "    learning_rate = learning_rate\n",
    "    iters = iters\n",
    "\n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = user_item_mat.shape[0]\n",
    "    n_offers = user_item_mat.shape[1]\n",
    "    num_offer_resp = np.count_nonzero(~np.isnan(user_item_mat))\n",
    "    \n",
    "    user_ids_series = np.array(user_item_df.index)\n",
    "    offer_labels_series = np.array(user_item_df.columns)\n",
    "\n",
    "    # initialize the user and offer matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    offer_mat = np.random.rand(latent_features, n_offers)\n",
    "\n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "\n",
    "    # keep track of iteration and MSE\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "\n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "\n",
    "        # For each user-offer pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_offers):\n",
    "\n",
    "                # if the offers count exists\n",
    "                if user_item_mat[i, j] > 0:\n",
    "\n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = user_item_mat[i, j] - np.dot(user_mat[i, :], offer_mat[:, j])\n",
    "\n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "\n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*offer_mat[k, j])\n",
    "                        offer_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "                        \n",
    "        # print results\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_offer_resp))\n",
    "\n",
    "    # SVD based fit\n",
    "    return user_mat, offer_mat, user_ids_series, offer_labels_series\n",
    "\n",
    "def predict_offer(customer_id, offer_label, uids=None, oids=None,\\\n",
    "                   user_mat=None, offer_mat=None):\n",
    "    '''\n",
    "    INPUT:\n",
    "    customer_id - the customer_id from the profile data frame \n",
    "    offer_label - the offer_label from the portfolio data frame \n",
    "\n",
    "    OUTPUT:\n",
    "    pred - the predicted rating for customer_id-offer_label according to FunkSVD\n",
    "    '''\n",
    "    try:# customer row and offer Column\n",
    "        customer_row = np.where(uids == customer_id)[0][0]\n",
    "        offer_col = np.where(oids == offer_label)[0][0]\n",
    "\n",
    "        # Take dot product of that row and column in U and V to make prediction\n",
    "        pred = math.floor(np.dot(user_mat[customer_row, :], offer_mat[:, offer_col]))\n",
    "\n",
    "        print(\"For customer {} we predict {} transactions for the offer {}.\".format(customer_id, round(pred), str(offer_label)))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    except:\n",
    "        print(\"I'm sorry, but a prediction cannot be made for this customer-offer pair.  It looks like one of these items does not exist in our current database.\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "def make_recommendations(customer_id=None, rec_num=1, uids=None, oids=None, \\\n",
    "                         user_mat=None, offer_mat=None, dfmco=None):\n",
    "    '''\n",
    "    INPUT:\n",
    "    rec_num - number of recommendations to return (int)\n",
    "\n",
    "    OUTPUT:\n",
    "    recs - (array) a list or numpy array of recommended offer for a customer_id given\n",
    "    '''\n",
    "    # if the customer id is available from the matrix factorization data,\n",
    "    # will use this and # of offer responses based on the predicted values\n",
    "    offer_labels = None\n",
    "    if customer_id != None:\n",
    "        if customer_id in uids:\n",
    "            # Get the index of which row the user is in for use in U matrix\n",
    "            idx = np.where(uids == customer_id)[0][0]\n",
    "\n",
    "            # take the dot product of that row and the V matrix\n",
    "            preds = np.dot(user_mat[idx,:],offer_mat)\n",
    "\n",
    "            # pull the top offers according to the prediction\n",
    "            indices = preds.argsort()[-rec_num:][::-1] #indices\n",
    "            offer_labels = oids[indices]\n",
    "        else:\n",
    "            # if we don't have this user, give just top ratings back\n",
    "            offer_labels = get_top_offers(n=rec_num, dfmco=dfmco)\n",
    "\n",
    "    else:\n",
    "        offer_labels = get_top_offers(n=rec_num, dfmco=dfmco)\n",
    "    return offer_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
